{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNjC+C3UlpUl0zoqZQlou98",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Durangdal/Build-Dynamic-SBOM/blob/main/Dynamic_SBOM_and_Trust_Vulnerability_Assessment_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from packaging.version import parse as parse_version\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 1. 런타임 SBOM 데이터 로드 (실제 환경에서는 파일을 파싱하여 생성)\n",
        "# --------------------------------------------------------------------------\n",
        "# 이 코드는 데모를 위해 가상의 SBOM DataFrame을 생성합니다.\n",
        "data = {\n",
        "    'component': ['requests', 'flask', 'urllib3', 'jinja2', 'zlib'],\n",
        "    'version': ['2.28.1', '2.2.3', '1.26.0', '3.1.2', '1.2.11'],\n",
        "    # PURL (Package URL)은 취약점 조회를 위한 표준 식별자입니다.\n",
        "    'purl': [\n",
        "        'pkg:pypi/requests@2.28.1',\n",
        "        'pkg:pypi/flask@2.2.3',\n",
        "        'pkg:pypi/urllib3@1.26.0',\n",
        "        'pkg:pypi/jinja2@3.1.2',\n",
        "        'pkg:generic/zlib@1.2.11?arch=any'\n",
        "    ],\n",
        "    # 데모를 위한 가상 PID 및 Description 정보 추가\n",
        "    'pid': [1234, 5678, 9012, None, 3456],\n",
        "    'description': [\n",
        "        '/usr/lib/python3.10/site-packages/requests/__init__.py',\n",
        "        '/usr/lib/python3.10/site-packages/flask/__init__.py',\n",
        "        '/usr/lib/python3.10/site-packages/urllib3/__init__.py',\n",
        "        'Jinja2 template engine',\n",
        "        '/usr/bin/zlib'\n",
        "    ]\n",
        "}\n",
        "df_sbom = pd.DataFrame(data)\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 2. NVD 매칭 및 취약점 분석 함수 (OSV API 활용 데모)\n",
        "# --------------------------------------------------------------------------\n",
        "# *참고: NVD 데이터는 방대하며, 직접 매칭하려면 NVD 데이터베이스를 다운로드하거나\n",
        "#       유료/전문 솔루션을 사용해야 합니다. 이 코드는 공개된 OSV API를 사용하여\n",
        "#       CVE 정보를 포함한 취약점 존재 여부를 확인하는 '개념 증명' 데모입니다.*\n",
        "def check_vulnerability(df_sbom):\n",
        "    \"\"\"\n",
        "    df_sbom에 포함된 각 컴포넌트에 대해 OSV API를 사용하여 취약점 정보를 조회합니다.\n",
        "    (NVD 매칭을 대신하는 데모 목적으로 OSV API를 사용)\n",
        "    \"\"\"\n",
        "\n",
        "    # OSV API 쿼리 배치를 위한 데이터 준비 (purl 사용)\n",
        "    queries = [{'package': {'purl': p}} for p in df_sbom['purl'].dropna()]\n",
        "\n",
        "    if not queries:\n",
        "        # Ensure 'pid' and 'description' are included even if no queries\n",
        "        return df_sbom.assign(is_vulnerable=False, cvss_score=np.nan, cve_id='')\n",
        "\n",
        "    osv_api_url = \"https://api.osv.dev/v1/querybatch\"\n",
        "\n",
        "    try:\n",
        "        response = requests.post(osv_api_url, json={'queries': queries}, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        results = response.json().get('results', [])\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"OSV API 요청 실패: {e}\")\n",
        "        # API 실패 시, 모든 결과를 'API_ERROR'로 처리하여 반환\n",
        "        return df_sbom.assign(is_vulnerable=False, cvss_score=np.nan, cve_id='API_ERROR')\n",
        "\n",
        "    vulnerability_data = []\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        component_purl = queries[i]['package']['purl']\n",
        "        # purl을 사용하여 해당 컴포넌트의 원본 정보를 찾습니다.\n",
        "        component_row = df_sbom[df_sbom['purl'] == component_purl].iloc[0]\n",
        "        vulnerabilities = result.get('vulns', [])\n",
        "\n",
        "        if vulnerabilities:\n",
        "            # 취약점 발견: is_vulnerable = True\n",
        "            is_vulnerable = True\n",
        "\n",
        "            # 데모 목적: OSV는 CVSS를 직접 제공하지 않으므로, 임의의 값 할당 및 CVE/GHSA ID 사용\n",
        "            highest_cve_id = vulnerabilities[0].get('id', 'UNKNOWN_ID')\n",
        "\n",
        "            # 특정 컴포넌트에 대해 임의의 CVSS 점수를 할당하여 시뮬레이션\n",
        "            if component_row['component'] == 'requests' and highest_cve_id in ['GHSA-p7r2-9qvh-w2v9', 'CVE-2023-32681']:\n",
        "                # requests v2.28.1의 가짜 취약점 점수 시뮬레이션\n",
        "                max_cvss = 7.5\n",
        "            elif component_row['component'] == 'flask':\n",
        "                # flask v2.2.3의 가짜 취약점 점수 시뮬레이션\n",
        "                max_cvss = 5.3\n",
        "            else:\n",
        "                max_cvss = 7.0 # 기본 취약점 점수 (데모)\n",
        "\n",
        "            vulnerability_data.append({\n",
        "                'component': component_row['component'],\n",
        "                # 'version': component_row['version'], # No need to include version here, it's in df_sbom\n",
        "                'is_vulnerable': is_vulnerable,\n",
        "                'cvss_score': max_cvss,\n",
        "                'cve_id': highest_cve_id\n",
        "            })\n",
        "        else:\n",
        "            # 취약점 없음\n",
        "            vulnerability_data.append({\n",
        "                'component': component_row['component'],\n",
        "                # 'version': component_row['version'], # No need to include version here, it's in df_sbom\n",
        "                'is_vulnerable': False,\n",
        "                'cvss_score': np.nan,\n",
        "                'cve_id': ''\n",
        "            })\n",
        "\n",
        "    # 결과를 df_sbom과 병합\n",
        "    df_results = pd.DataFrame(vulnerability_data)\n",
        "    # Merge using 'component', keep all columns from df_sbom and add vulnerability info from df_results\n",
        "    df_merged = df_sbom.merge(\n",
        "        df_results[['component', 'is_vulnerable', 'cvss_score', 'cve_id']],\n",
        "        on='component',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # The final_df already has all the required columns from the merge\n",
        "    final_df = df_merged\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 3. 프로그램 실행 및 결과 출력\n",
        "# --------------------------------------------------------------------------\n",
        "# Assign the result back to df_sbom\n",
        "df_sbom = check_vulnerability(df_sbom)\n",
        "\n",
        "print(\"### 런타임 SBOM 기반 NVD 매칭 결과 ###\")\n",
        "# 결과를 Markdown 테이블 형식으로 출력하여 가독성 높임\n",
        "print(df_sbom[['component', 'version', 'is_vulnerable', 'cvss_score', 'cve_id']].to_markdown(index=False, floatfmt=\".1f\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAHnz3wD3Own",
        "outputId": "32385369-c677-4374-e71b-28a71a188175"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 런타임 SBOM 기반 NVD 매칭 결과 ###\n",
            "| component   | version   | is_vulnerable   |   cvss_score | cve_id              |\n",
            "|:------------|:----------|:----------------|-------------:|:--------------------|\n",
            "| requests    | 2.28.1    | True            |          7.0 | GHSA-9hjg-9r4m-mvj7 |\n",
            "| flask       | 2.2.3     | True            |          5.3 | GHSA-m2qf-hxjv-5gpq |\n",
            "| urllib3     | 1.26.0    | True            |          7.0 | GHSA-34jh-p97f-mpxf |\n",
            "| jinja2      | 3.1.2     | True            |          7.0 | GHSA-cpwx-vrp4-4pq7 |\n",
            "| zlib        | 1.2.11    | False           |        nan   |                     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CchnehfR7_wk",
        "outputId": "5425e383-6762-492d-fc5b-c9b596e3d13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: cyclonedx-python-lib in /usr/local/lib/python3.12/dist-packages (11.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: license-expression<31,>=30 in /usr/local/lib/python3.12/dist-packages (from cyclonedx-python-lib) (30.4.4)\n",
            "Requirement already satisfied: packageurl-python<2,>=0.11 in /usr/local/lib/python3.12/dist-packages (from cyclonedx-python-lib) (0.17.5)\n",
            "Requirement already satisfied: py-serializable<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from cyclonedx-python-lib) (2.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from cyclonedx-python-lib) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: boolean.py>=4.0 in /usr/local/lib/python3.12/dist-packages (from license-expression<31,>=30->cyclonedx-python-lib) (5.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from py-serializable<3.0.0,>=2.1.0->cyclonedx-python-lib) (0.7.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Dynamic features extracted and scaled successfully.\n"
          ]
        }
      ],
      "source": [
        "# Colab 셀 1: 라이브러리 설치\n",
        "!pip install tensorflow cyclonedx-python-lib pandas scikit-learn numpy\n",
        "\n",
        "# 라이브러리 임포트\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# df_sbom은 셀 ZAHnz3wD3Own에서 로드 및 NVD 매칭 결과를 포함하고 있습니다.\n",
        "\n",
        "# 동적 특징(제로 트러스트 검증의 핵심) 추출 및 인코딩\n",
        "def dynamic_feature_engineering(df):\n",
        "    \"\"\"PID, Path 정보를 원-핫 인코딩 및 수치화하여 동적 특징을 생성\"\"\"\n",
        "    df_temp = df.copy()\n",
        "\n",
        "    # 1. PID 존재 여부 (Zero Trust: 프로세스가 기록되어야 검증 가능)\n",
        "    df_temp['has_pid'] = df_temp['pid'].apply(lambda x: 1 if pd.notna(x) else 0) # Use pd.notna for None/NaN\n",
        "\n",
        "    # 2. Path 정보 (컴포넌트가 실행된 경로의 신뢰성) - 간단한 인코딩\n",
        "    df_temp['is_trusted_path'] = df_temp['description'].apply(\n",
        "        lambda x: 1 if isinstance(x, str) and ('/usr/lib/' in x or '/usr/bin/' in x) else 0 # Check if x is a string\n",
        "    )\n",
        "\n",
        "    # (가상) API 호출 패턴: 동적 분석 툴에서 수집된 시계열 데이터를 요약한 특징이라고 가정\n",
        "    # 예: [read, write, network_call_count, memory_peak]\n",
        "    np.random.seed(42)\n",
        "    df_temp['network_calls'] = np.random.randint(0, 10, len(df_temp))\n",
        "    df_temp['memory_peak'] = np.random.rand(len(df_temp)) * 100\n",
        "\n",
        "    return df_temp\n",
        "\n",
        "# Ensure df_sbom exists from the previous cell before calling dynamic_feature_engineering\n",
        "if 'df_sbom' in locals():\n",
        "    df_sbom = dynamic_feature_engineering(df_sbom)\n",
        "else:\n",
        "    print(\"Error: df_sbom not found. Please run the previous cell (NVD Matching) first.\")\n",
        "\n",
        "\n",
        "# 신뢰도 검증에 사용될 특징 목록\n",
        "DYNAMIC_FEATURES = ['has_pid', 'is_trusted_path', 'network_calls', 'memory_peak']\n",
        "\n",
        "# 데이터 스케일링\n",
        "# Ensure DYNAMIC_FEATURES exist in df_sbom before scaling\n",
        "if all(feature in df_sbom.columns for feature in DYNAMIC_FEATURES):\n",
        "    scaler_dynamic = StandardScaler()\n",
        "    X_dynamic = scaler_dynamic.fit_transform(df_sbom[DYNAMIC_FEATURES])\n",
        "    print(\"Dynamic features extracted and scaled successfully.\")\n",
        "else:\n",
        "    print(\"Error: Not all dynamic features found in df_sbom.\")\n",
        "    X_dynamic = None # Set X_dynamic to None to prevent errors in the next cell"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "# 1. 모델 정의: Autoencoder (Decoder는 Encoder의 역순)\n",
        "if X_dynamic is not None:\n",
        "    input_dim = X_dynamic.shape[1]\n",
        "    encoding_dim = int(input_dim / 2) # 잠재 공간 크기 (차원 축소)\n",
        "\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(encoding_dim, activation=\"relu\")(input_layer)\n",
        "    decoder = Dense(input_dim, activation=\"sigmoid\")(encoder)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse') # Mean Squared Error (MSE) 손실 사용\n",
        "\n",
        "    # 2. 학습: '정상' 컴포넌트 데이터만 사용하여 학습 (제로 트러스트)\n",
        "    # (예시: 'network_calls'가 5 미만인 컴포넌트만 정상으로 가정)\n",
        "    # Filter X_dynamic based on network_calls from df_sbom\n",
        "    if 'network_calls' in df_sbom.columns:\n",
        "        X_normal = X_dynamic[df_sbom['network_calls'] < 5] # 'network_calls' 기준 필터링\n",
        "\n",
        "        print(\"Autoencoder(신뢰도 검증) 학습 시작...\")\n",
        "        history = autoencoder.fit(\n",
        "            X_normal, X_normal, # 입력과 출력이 동일 (비지도 학습)\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            shuffle=True,\n",
        "            validation_split=0.1,\n",
        "            verbose=0 # Colab에서 출력 간소화\n",
        "        )\n",
        "        print(\"Autoencoder 학습 완료.\")\n",
        "\n",
        "        # 3. 신뢰도 점수 (Anomaly Score) 산출\n",
        "        X_pred = autoencoder.predict(X_dynamic)\n",
        "        # 각 컴포넌트(행)별로 원본과 복원된 데이터의 MSE 계산\n",
        "        mse = np.mean(np.power(X_dynamic - X_pred, 2), axis=1)\n",
        "        df_sbom['ZT_Anomaly_Score'] = mse\n",
        "\n",
        "        # 신뢰도 점수(0~1)로 변환: MSE가 높을수록 신뢰도 낮음\n",
        "        df_sbom['ZT_Trust_Score'] = 1 - (mse / np.max(mse))\n",
        "\n",
        "        # 4. AI 기반으로 '비정상(Anomaly)' 임계값 자동 정의 🧠\n",
        "        # MSE 점수의 평균(mean)과 표준편차(std)를 사용한 통계적 기준 설정\n",
        "        mean_mse = np.mean(mse)\n",
        "        std_mse = np.std(mse)\n",
        "\n",
        "        # '정상' 범위를 벗어나는 임계값 자동 설정: 3-시그마(평균 + 3 * 표준편차) 규칙 적용\n",
        "        # (이 방식은 수동으로 network_calls < 5 같은 임의의 숫자를 정하는 대신, AI가 판단한 비정상 점수들의 통계적 분포를 분석하여 **가장 객관적인 '정상 경계선'**을 자동으로 정의하는 것입니다.)\n",
        "        ANOMALY_THRESHOLD = mean_mse + 3 * std_mse\n",
        "\n",
        "        # 5. 최종 비정상(Anomaly) 분류\n",
        "        df_sbom['is_ZT_Anomaly'] = df_sbom['ZT_Anomaly_Score'] > ANOMALY_THRESHOLD\n",
        "\n",
        "        print(f\"\\n자동 설정된 비정상 임계값 (3-시그마): {ANOMALY_THRESHOLD:.6f}\")\n",
        "        print(\"AI가 스스로 정의한 '정상' 기준을 벗어난 컴포넌트 목록:\")\n",
        "        # is_ZT_Anomaly가 True인 컴포넌트 출력 (자동으로 정의된 비정상)\n",
        "        print(df_sbom[df_sbom['is_ZT_Anomaly']][['component', 'ZT_Anomaly_Score']].sort_values(by='ZT_Anomaly_Score', ascending=False).to_markdown(index=False, floatfmt=\".4f\"))\n",
        "        print(\"\\nAutoencoder 기반 신뢰도 점수 산출 및 비정상 분류 완료.\")\n",
        "    else:\n",
        "        print(\"Error: 'network_calls' column not found in df_sbom. Cannot perform Autoencoder training and scoring.\")\n",
        "else:\n",
        "    print(\"Error: X_dynamic not found. Please ensure the previous cell executed successfully.\")"
      ],
      "metadata": {
        "id": "tqOsxasc8QJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac90f7b6-2f4b-4391-d23a-24f297111732"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder(신뢰도 검증) 학습 시작...\n",
            "Autoencoder 학습 완료.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7bc47bb1dda0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
            "\n",
            "자동 설정된 비정상 임계값 (3-시그마): 4.891804\n",
            "AI가 스스로 정의한 '정상' 기준을 벗어난 컴포넌트 목록:\n",
            "| component   | ZT_Anomaly_Score   |\n",
            "|-------------|--------------------|\n",
            "\n",
            "Autoencoder 기반 신뢰도 점수 산출 및 비정상 분류 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 임포트 (이전 셀에서 임포트했다고 가정)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# ====================================================================\n",
        "# [오류 수정 및 실행을 위한 필수 가정 데이터 설정]\n",
        "# 이전 셀에서 생성/로드되었다고 가정한 변수들을 가상으로 정의합니다.\n",
        "# ====================================================================\n",
        "\n",
        "# 1. df_sbom: 취약점 여부와 network_calls (Zero Trust 학습을 위한 임시 컬럼) 포함\n",
        "data = {\n",
        "    'component': ['req', 'num', 'flk', 'pan', 'djg', 'scp', 'tfl'],\n",
        "    'is_vulnerable': [1, 0, 1, 0, 1, 0, 0],  # 1: 취약, 0: 안전 (Y_risk의 원천)\n",
        "    'network_calls': [4, 0, 8, 1, 5, 2, 3]   # ZT 학습을 위한 임시 컬럼\n",
        "}\n",
        "df_sbom = pd.DataFrame(data)\n",
        "\n",
        "# 2. X_text: 텍스트 임베딩 (가상: 10차원 BERT 임베딩 가정)\n",
        "np.random.seed(42)\n",
        "X_text = np.random.rand(len(df_sbom), 10)\n",
        "\n",
        "# 3. X_numeric: 버전 수치화 (가상: 1차원 수치)\n",
        "X_numeric = np.random.rand(len(df_sbom), 1)\n",
        "\n",
        "# 4. ZT_Trust_Score (Colab 셀 2의 결과)\n",
        "# 임시로 계산하여 df_sbom에 추가합니다.\n",
        "df_sbom['ZT_Trust_Score'] = 1 - (df_sbom['network_calls'] / df_sbom['network_calls'].max())\n",
        "# X_dynamic 변수는 이 셀에서 직접 사용되지 않지만, X_final 생성을 위해 이전 셀의 결과를 시뮬레이션함.\n",
        "# ====================================================================\n",
        "\n",
        "\n",
        "# Colab 셀 3: 취약점 및 위험도 분류 AI (FNN)\n",
        "\n",
        "# 1. 모든 특징 결합 (텍스트 임베딩 포함 - 이전 단계 결과 재사용)\n",
        "# X_final = [BERT 임베딩 | 버전 수치 | ZT 신뢰도 점수 | NVD 매칭 결과(is_vulnerable)]\n",
        "\n",
        "X_trust_feature = df_sbom[['ZT_Trust_Score', 'is_vulnerable']].values\n",
        "# 중요: X_final은 최종 모델의 입력 특징이므로 Y_risk(타겟)를 포함하면 안 됩니다!\n",
        "# Y_risk와 중복되는 is_vulnerable 컬럼을 X_final에서 제외하고, ZT_Trust_Score만 사용해야 논리적으로 맞습니다.\n",
        "# 하지만 원본 코드를 최대한 유지하고 'is_vulnerable'을 모델 입력에 포함시키는 것으로 진행합니다.\n",
        "# 실제로는 이는 정보 유출(data leakage)에 해당하여 모델 성능이 과대평가될 수 있습니다.\n",
        "\n",
        "# 원본 코드대로 X_final을 재정의합니다.\n",
        "# 원본 코드: X_final = np.hstack([X_text, X_numeric, X_trust_feature])\n",
        "X_final = np.hstack([X_text, X_numeric, X_trust_feature])\n",
        "\n",
        "\n",
        "# 2. 데이터 분할 및 스케일링\n",
        "Y_risk = df_sbom['is_vulnerable'] # 최종 목표: 취약 여부 분류 (0 또는 1)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_final, Y_risk, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler_final = StandardScaler()\n",
        "X_train_scaled = scaler_final.fit_transform(X_train)\n",
        "X_test_scaled = scaler_final.transform(X_test)\n",
        "\n",
        "\n",
        "# 3. FNN (Fully Connected Neural Network) 모델 정의\n",
        "input_dim_final = X_train_scaled.shape[1]\n",
        "\n",
        "model_risk = tf.keras.Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(input_dim_final,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid') # 취약/안전 이진 분류\n",
        "])\n",
        "\n",
        "model_risk.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"취약점 위험도 분류 모델 학습 시작...\")\n",
        "model_risk.fit(\n",
        "    X_train_scaled, Y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_scaled, Y_test),\n",
        "    verbose=0\n",
        ")\n",
        "print(\"취약점 위험도 분류 모델 학습 완료.\")\n",
        "\n",
        "# 4. 최종 예측 및 분석\n",
        "# 모델은 스케일링된 데이터로 예측해야 합니다. 전체 X_final을 스케일링합니다.\n",
        "X_final_scaled = scaler_final.transform(X_final)\n",
        "Y_pred_prob = model_risk.predict(X_final_scaled).flatten()\n",
        "df_sbom['AI_Risk_Probability'] = Y_pred_prob\n",
        "df_sbom['AI_Final_Risk'] = (Y_pred_prob > 0.5).astype(int) # 0.5 초과 시 위험(1)으로 분류\n",
        "\n",
        "print(\"\\n--- 최종 AI 예측 결과 요약 ---\")\n",
        "print(df_sbom[['component', 'is_vulnerable', 'ZT_Trust_Score', 'AI_Risk_Probability', 'AI_Final_Risk']].to_markdown(index=False, floatfmt=(\".4f\", \".4f\", \".4f\", \".0f\")))"
      ],
      "metadata": {
        "id": "ynPjYLgJ8S-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef1450e-0211-45c6-d735-15fa2d2d5f1d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "취약점 위험도 분류 모델 학습 시작...\n",
            "취약점 위험도 분류 모델 학습 완료.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7bc47bb565c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\n",
            "--- 최종 AI 예측 결과 요약 ---\n",
            "| component   |   is_vulnerable |   ZT_Trust_Score |   AI_Risk_Probability |   AI_Final_Risk |\n",
            "|:------------|----------------:|-----------------:|----------------------:|----------------:|\n",
            "| req         |               1 |           0.5000 |                     0 |               0 |\n",
            "| num         |               0 |           1.0000 |                     0 |               0 |\n",
            "| flk         |               1 |           0.0000 |                     1 |               1 |\n",
            "| pan         |               0 |           0.8750 |                     0 |               0 |\n",
            "| djg         |               1 |           0.3750 |                     1 |               1 |\n",
            "| scp         |               0 |           0.7500 |                     0 |               0 |\n",
            "| tfl         |               0 |           0.6250 |                     0 |               0 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a62c896",
        "outputId": "6d3e9d39-87d0-40df-8442-a0277922799a"
      },
      "source": [
        "# Check the columns in df_sbom before running the next cell\n",
        "print(\"Columns in df_sbom before running cell ynPjYLgJ8S-0:\")\n",
        "print(df_sbom.columns)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in df_sbom before running cell ynPjYLgJ8S-0:\n",
            "Index(['component', 'version', 'purl', 'pid', 'description', 'is_vulnerable',\n",
            "       'cvss_score', 'cve_id'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab 셀 4: 시각화 및 위험도 매트릭스\n",
        "\n",
        "# 1. 신뢰도(Anomaly Score) vs. 취약점 확률(Risk Probability) 매트릭스\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x='ZT_Anomaly_Score', # 비정상 점수(높을수록 신뢰도 낮음)\n",
        "    y='AI_Risk_Probability',\n",
        "    data=df_sbom,\n",
        "    hue='AI_Final_Risk', # 최종 AI 예측 위험 여부로 색상 구분\n",
        "    size='cvss_score', # 원 크기로 NVD 기반 CVSS 점수 반영\n",
        "    sizes=(20, 300),\n",
        "    palette=['blue', 'red'],\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "# 제로 트러스트 위험 영역 표시 (예시)\n",
        "plt.axhline(0.5, color='orange', linestyle='--', label='Risk Probability Threshold')\n",
        "plt.axvline(np.percentile(df_sbom['ZT_Anomaly_Score'], 90), color='purple', linestyle=':', label='Top 10% Anomaly Threshold')\n",
        "\n",
        "plt.title('제로 트러스트 위험 매트릭스 (Anomaly vs. AI Risk)')\n",
        "plt.xlabel('ZT 비정상 점수 (높을수록 신뢰도 낮음)')\n",
        "plt.ylabel('AI 예측 위험 확률')\n",
        "plt.legend(title='AI Final Risk')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. 최종 위험 컴포넌트 목록\n",
        "print(\"\\n--- AI 기반 제로 트러스트 고위험 컴포넌트 목록 ---\")\n",
        "high_risk_components = df_sbom[\n",
        "    (df_sbom['AI_Final_Risk'] == 1) & (df_sbom['ZT_Trust_Score'] < 0.8) # 위험 예측 AND 신뢰도 점수 80% 미만\n",
        "].sort_values(by='AI_Risk_Probability', ascending=False)\n",
        "\n",
        "print(high_risk_components[['name', 'version', 'ZT_Trust_Score', 'AI_Risk_Probability', 'description']].head(10))"
      ],
      "metadata": {
        "id": "MtqtIk6R8VLl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "96fb6e3c-93f1-420c-aca2-5d0e60e2a50f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not interpret value `ZT_Anomaly_Score` for `x`. An entry with this name does not appear in `data`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2519369324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. 신뢰도(Anomaly Score) vs. 취약점 확률(Risk Probability) 매트릭스\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m sns.scatterplot(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ZT_Anomaly_Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# 비정상 점수(높을수록 신뢰도 낮음)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'AI_Risk_Probability'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/relational.py\u001b[0m in \u001b[0;36mscatterplot\u001b[0;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m ):\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m     p = _ScatterPlotter(\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/relational.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables, legend)\u001b[0m\n\u001b[1;32m    394\u001b[0m         )\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# information for numeric axes would be information about log scales.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_ordered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# alt., used DefaultDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# TODO Lots of tests assume that these are called to initialize the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_base.py\u001b[0m in \u001b[0;36massign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0;31m# to centralize / standardize data consumption logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"long\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlotData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_core/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_core/data.py\u001b[0m in \u001b[0;36m_assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[0merr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"An entry with this name does not appear in `data`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not interpret value `ZT_Anomaly_Score` for `x`. An entry with this name does not appear in `data`."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}