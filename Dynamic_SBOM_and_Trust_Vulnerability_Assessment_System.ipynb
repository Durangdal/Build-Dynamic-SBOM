{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNjC+C3UlpUl0zoqZQlou98",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Durangdal/Build-Dynamic-SBOM/blob/main/Dynamic_SBOM_and_Trust_Vulnerability_Assessment_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from packaging.version import parse as parse_version\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 1. ëŸ°íƒ€ì„ SBOM ë°ì´í„° ë¡œë“œ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ìƒì„±)\n",
        "# --------------------------------------------------------------------------\n",
        "# ì´ ì½”ë“œëŠ” ë°ëª¨ë¥¼ ìœ„í•´ ê°€ìƒì˜ SBOM DataFrameì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "data = {\n",
        "    'component': ['requests', 'flask', 'urllib3', 'jinja2', 'zlib'],\n",
        "    'version': ['2.28.1', '2.2.3', '1.26.0', '3.1.2', '1.2.11'],\n",
        "    # PURL (Package URL)ì€ ì·¨ì•½ì  ì¡°íšŒë¥¼ ìœ„í•œ í‘œì¤€ ì‹ë³„ìì…ë‹ˆë‹¤.\n",
        "    'purl': [\n",
        "        'pkg:pypi/requests@2.28.1',\n",
        "        'pkg:pypi/flask@2.2.3',\n",
        "        'pkg:pypi/urllib3@1.26.0',\n",
        "        'pkg:pypi/jinja2@3.1.2',\n",
        "        'pkg:generic/zlib@1.2.11?arch=any'\n",
        "    ],\n",
        "    # ë°ëª¨ë¥¼ ìœ„í•œ ê°€ìƒ PID ë° Description ì •ë³´ ì¶”ê°€\n",
        "    'pid': [1234, 5678, 9012, None, 3456],\n",
        "    'description': [\n",
        "        '/usr/lib/python3.10/site-packages/requests/__init__.py',\n",
        "        '/usr/lib/python3.10/site-packages/flask/__init__.py',\n",
        "        '/usr/lib/python3.10/site-packages/urllib3/__init__.py',\n",
        "        'Jinja2 template engine',\n",
        "        '/usr/bin/zlib'\n",
        "    ]\n",
        "}\n",
        "df_sbom = pd.DataFrame(data)\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 2. NVD ë§¤ì¹­ ë° ì·¨ì•½ì  ë¶„ì„ í•¨ìˆ˜ (OSV API í™œìš© ë°ëª¨)\n",
        "# --------------------------------------------------------------------------\n",
        "# *ì°¸ê³ : NVD ë°ì´í„°ëŠ” ë°©ëŒ€í•˜ë©°, ì§ì ‘ ë§¤ì¹­í•˜ë ¤ë©´ NVD ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜\n",
        "#       ìœ ë£Œ/ì „ë¬¸ ì†”ë£¨ì…˜ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ì½”ë“œëŠ” ê³µê°œëœ OSV APIë¥¼ ì‚¬ìš©í•˜ì—¬\n",
        "#       CVE ì •ë³´ë¥¼ í¬í•¨í•œ ì·¨ì•½ì  ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” 'ê°œë… ì¦ëª…' ë°ëª¨ì…ë‹ˆë‹¤.*\n",
        "def check_vulnerability(df_sbom):\n",
        "    \"\"\"\n",
        "    df_sbomì— í¬í•¨ëœ ê° ì»´í¬ë„ŒíŠ¸ì— ëŒ€í•´ OSV APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì·¨ì•½ì  ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\n",
        "    (NVD ë§¤ì¹­ì„ ëŒ€ì‹ í•˜ëŠ” ë°ëª¨ ëª©ì ìœ¼ë¡œ OSV APIë¥¼ ì‚¬ìš©)\n",
        "    \"\"\"\n",
        "\n",
        "    # OSV API ì¿¼ë¦¬ ë°°ì¹˜ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ (purl ì‚¬ìš©)\n",
        "    queries = [{'package': {'purl': p}} for p in df_sbom['purl'].dropna()]\n",
        "\n",
        "    if not queries:\n",
        "        # Ensure 'pid' and 'description' are included even if no queries\n",
        "        return df_sbom.assign(is_vulnerable=False, cvss_score=np.nan, cve_id='')\n",
        "\n",
        "    osv_api_url = \"https://api.osv.dev/v1/querybatch\"\n",
        "\n",
        "    try:\n",
        "        response = requests.post(osv_api_url, json={'queries': queries}, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        results = response.json().get('results', [])\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"OSV API ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
        "        # API ì‹¤íŒ¨ ì‹œ, ëª¨ë“  ê²°ê³¼ë¥¼ 'API_ERROR'ë¡œ ì²˜ë¦¬í•˜ì—¬ ë°˜í™˜\n",
        "        return df_sbom.assign(is_vulnerable=False, cvss_score=np.nan, cve_id='API_ERROR')\n",
        "\n",
        "    vulnerability_data = []\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        component_purl = queries[i]['package']['purl']\n",
        "        # purlì„ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ì»´í¬ë„ŒíŠ¸ì˜ ì›ë³¸ ì •ë³´ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
        "        component_row = df_sbom[df_sbom['purl'] == component_purl].iloc[0]\n",
        "        vulnerabilities = result.get('vulns', [])\n",
        "\n",
        "        if vulnerabilities:\n",
        "            # ì·¨ì•½ì  ë°œê²¬: is_vulnerable = True\n",
        "            is_vulnerable = True\n",
        "\n",
        "            # ë°ëª¨ ëª©ì : OSVëŠ” CVSSë¥¼ ì§ì ‘ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì„ì˜ì˜ ê°’ í• ë‹¹ ë° CVE/GHSA ID ì‚¬ìš©\n",
        "            highest_cve_id = vulnerabilities[0].get('id', 'UNKNOWN_ID')\n",
        "\n",
        "            # íŠ¹ì • ì»´í¬ë„ŒíŠ¸ì— ëŒ€í•´ ì„ì˜ì˜ CVSS ì ìˆ˜ë¥¼ í• ë‹¹í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜\n",
        "            if component_row['component'] == 'requests' and highest_cve_id in ['GHSA-p7r2-9qvh-w2v9', 'CVE-2023-32681']:\n",
        "                # requests v2.28.1ì˜ ê°€ì§œ ì·¨ì•½ì  ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜\n",
        "                max_cvss = 7.5\n",
        "            elif component_row['component'] == 'flask':\n",
        "                # flask v2.2.3ì˜ ê°€ì§œ ì·¨ì•½ì  ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜\n",
        "                max_cvss = 5.3\n",
        "            else:\n",
        "                max_cvss = 7.0 # ê¸°ë³¸ ì·¨ì•½ì  ì ìˆ˜ (ë°ëª¨)\n",
        "\n",
        "            vulnerability_data.append({\n",
        "                'component': component_row['component'],\n",
        "                # 'version': component_row['version'], # No need to include version here, it's in df_sbom\n",
        "                'is_vulnerable': is_vulnerable,\n",
        "                'cvss_score': max_cvss,\n",
        "                'cve_id': highest_cve_id\n",
        "            })\n",
        "        else:\n",
        "            # ì·¨ì•½ì  ì—†ìŒ\n",
        "            vulnerability_data.append({\n",
        "                'component': component_row['component'],\n",
        "                # 'version': component_row['version'], # No need to include version here, it's in df_sbom\n",
        "                'is_vulnerable': False,\n",
        "                'cvss_score': np.nan,\n",
        "                'cve_id': ''\n",
        "            })\n",
        "\n",
        "    # ê²°ê³¼ë¥¼ df_sbomê³¼ ë³‘í•©\n",
        "    df_results = pd.DataFrame(vulnerability_data)\n",
        "    # Merge using 'component', keep all columns from df_sbom and add vulnerability info from df_results\n",
        "    df_merged = df_sbom.merge(\n",
        "        df_results[['component', 'is_vulnerable', 'cvss_score', 'cve_id']],\n",
        "        on='component',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # The final_df already has all the required columns from the merge\n",
        "    final_df = df_merged\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# 3. í”„ë¡œê·¸ë¨ ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
        "# --------------------------------------------------------------------------\n",
        "# Assign the result back to df_sbom\n",
        "df_sbom = check_vulnerability(df_sbom)\n",
        "\n",
        "print(\"### ëŸ°íƒ€ì„ SBOM ê¸°ë°˜ NVD ë§¤ì¹­ ê²°ê³¼ ###\")\n",
        "# ê²°ê³¼ë¥¼ Markdown í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì—¬ ê°€ë…ì„± ë†’ì„\n",
        "print(df_sbom[['component', 'version', 'is_vulnerable', 'cvss_score', 'cve_id']].to_markdown(index=False, floatfmt=\".1f\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAHnz3wD3Own",
        "outputId": "32385369-c677-4374-e71b-28a71a188175"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### ëŸ°íƒ€ì„ SBOM ê¸°ë°˜ NVD ë§¤ì¹­ ê²°ê³¼ ###\n",
            "| component   | version   | is_vulnerable   |   cvss_score | cve_id              |\n",
            "|:------------|:----------|:----------------|-------------:|:--------------------|\n",
            "| requests    | 2.28.1    | True            |          7.0 | GHSA-9hjg-9r4m-mvj7 |\n",
            "| flask       | 2.2.3     | True            |          5.3 | GHSA-m2qf-hxjv-5gpq |\n",
            "| urllib3     | 1.26.0    | True            |          7.0 | GHSA-34jh-p97f-mpxf |\n",
            "| jinja2      | 3.1.2     | True            |          7.0 | GHSA-cpwx-vrp4-4pq7 |\n",
            "| zlib        | 1.2.11    | False           |        nan   |                     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CchnehfR7_wk",
        "outputId": "5425e383-6762-492d-fc5b-c9b596e3d13c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: cyclonedx-python-lib in /usr/local/lib/python3.12/dist-packages (11.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: license-expression<31,>=30 in /usr/local/lib/python3.12/dist-packages (from cyclonedx-python-lib) (30.4.4)\n",
            "Requirement already satisfied: packageurl-python<2,>=0.11 in /usr/local/lib/python3.12/dist-packages (from cyclonedx-python-lib) (0.17.5)\n",
            "Requirement already satisfied: py-serializable<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from cyclonedx-python-lib) (2.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from cyclonedx-python-lib) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: boolean.py>=4.0 in /usr/local/lib/python3.12/dist-packages (from license-expression<31,>=30->cyclonedx-python-lib) (5.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from py-serializable<3.0.0,>=2.1.0->cyclonedx-python-lib) (0.7.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Dynamic features extracted and scaled successfully.\n"
          ]
        }
      ],
      "source": [
        "# Colab ì…€ 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install tensorflow cyclonedx-python-lib pandas scikit-learn numpy\n",
        "\n",
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# df_sbomì€ ì…€ ZAHnz3wD3Ownì—ì„œ ë¡œë“œ ë° NVD ë§¤ì¹­ ê²°ê³¼ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "# ë™ì  íŠ¹ì§•(ì œë¡œ íŠ¸ëŸ¬ìŠ¤íŠ¸ ê²€ì¦ì˜ í•µì‹¬) ì¶”ì¶œ ë° ì¸ì½”ë”©\n",
        "def dynamic_feature_engineering(df):\n",
        "    \"\"\"PID, Path ì •ë³´ë¥¼ ì›-í•« ì¸ì½”ë”© ë° ìˆ˜ì¹˜í™”í•˜ì—¬ ë™ì  íŠ¹ì§•ì„ ìƒì„±\"\"\"\n",
        "    df_temp = df.copy()\n",
        "\n",
        "    # 1. PID ì¡´ì¬ ì—¬ë¶€ (Zero Trust: í”„ë¡œì„¸ìŠ¤ê°€ ê¸°ë¡ë˜ì–´ì•¼ ê²€ì¦ ê°€ëŠ¥)\n",
        "    df_temp['has_pid'] = df_temp['pid'].apply(lambda x: 1 if pd.notna(x) else 0) # Use pd.notna for None/NaN\n",
        "\n",
        "    # 2. Path ì •ë³´ (ì»´í¬ë„ŒíŠ¸ê°€ ì‹¤í–‰ëœ ê²½ë¡œì˜ ì‹ ë¢°ì„±) - ê°„ë‹¨í•œ ì¸ì½”ë”©\n",
        "    df_temp['is_trusted_path'] = df_temp['description'].apply(\n",
        "        lambda x: 1 if isinstance(x, str) and ('/usr/lib/' in x or '/usr/bin/' in x) else 0 # Check if x is a string\n",
        "    )\n",
        "\n",
        "    # (ê°€ìƒ) API í˜¸ì¶œ íŒ¨í„´: ë™ì  ë¶„ì„ íˆ´ì—ì„œ ìˆ˜ì§‘ëœ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìš”ì•½í•œ íŠ¹ì§•ì´ë¼ê³  ê°€ì •\n",
        "    # ì˜ˆ: [read, write, network_call_count, memory_peak]\n",
        "    np.random.seed(42)\n",
        "    df_temp['network_calls'] = np.random.randint(0, 10, len(df_temp))\n",
        "    df_temp['memory_peak'] = np.random.rand(len(df_temp)) * 100\n",
        "\n",
        "    return df_temp\n",
        "\n",
        "# Ensure df_sbom exists from the previous cell before calling dynamic_feature_engineering\n",
        "if 'df_sbom' in locals():\n",
        "    df_sbom = dynamic_feature_engineering(df_sbom)\n",
        "else:\n",
        "    print(\"Error: df_sbom not found. Please run the previous cell (NVD Matching) first.\")\n",
        "\n",
        "\n",
        "# ì‹ ë¢°ë„ ê²€ì¦ì— ì‚¬ìš©ë  íŠ¹ì§• ëª©ë¡\n",
        "DYNAMIC_FEATURES = ['has_pid', 'is_trusted_path', 'network_calls', 'memory_peak']\n",
        "\n",
        "# ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
        "# Ensure DYNAMIC_FEATURES exist in df_sbom before scaling\n",
        "if all(feature in df_sbom.columns for feature in DYNAMIC_FEATURES):\n",
        "    scaler_dynamic = StandardScaler()\n",
        "    X_dynamic = scaler_dynamic.fit_transform(df_sbom[DYNAMIC_FEATURES])\n",
        "    print(\"Dynamic features extracted and scaled successfully.\")\n",
        "else:\n",
        "    print(\"Error: Not all dynamic features found in df_sbom.\")\n",
        "    X_dynamic = None # Set X_dynamic to None to prevent errors in the next cell"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "# 1. ëª¨ë¸ ì •ì˜: Autoencoder (DecoderëŠ” Encoderì˜ ì—­ìˆœ)\n",
        "if X_dynamic is not None:\n",
        "    input_dim = X_dynamic.shape[1]\n",
        "    encoding_dim = int(input_dim / 2) # ì ì¬ ê³µê°„ í¬ê¸° (ì°¨ì› ì¶•ì†Œ)\n",
        "\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(encoding_dim, activation=\"relu\")(input_layer)\n",
        "    decoder = Dense(input_dim, activation=\"sigmoid\")(encoder)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "    autoencoder.compile(optimizer='adam', loss='mse') # Mean Squared Error (MSE) ì†ì‹¤ ì‚¬ìš©\n",
        "\n",
        "    # 2. í•™ìŠµ: 'ì •ìƒ' ì»´í¬ë„ŒíŠ¸ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ (ì œë¡œ íŠ¸ëŸ¬ìŠ¤íŠ¸)\n",
        "    # (ì˜ˆì‹œ: 'network_calls'ê°€ 5 ë¯¸ë§Œì¸ ì»´í¬ë„ŒíŠ¸ë§Œ ì •ìƒìœ¼ë¡œ ê°€ì •)\n",
        "    # Filter X_dynamic based on network_calls from df_sbom\n",
        "    if 'network_calls' in df_sbom.columns:\n",
        "        X_normal = X_dynamic[df_sbom['network_calls'] < 5] # 'network_calls' ê¸°ì¤€ í•„í„°ë§\n",
        "\n",
        "        print(\"Autoencoder(ì‹ ë¢°ë„ ê²€ì¦) í•™ìŠµ ì‹œì‘...\")\n",
        "        history = autoencoder.fit(\n",
        "            X_normal, X_normal, # ì…ë ¥ê³¼ ì¶œë ¥ì´ ë™ì¼ (ë¹„ì§€ë„ í•™ìŠµ)\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            shuffle=True,\n",
        "            validation_split=0.1,\n",
        "            verbose=0 # Colabì—ì„œ ì¶œë ¥ ê°„ì†Œí™”\n",
        "        )\n",
        "        print(\"Autoencoder í•™ìŠµ ì™„ë£Œ.\")\n",
        "\n",
        "        # 3. ì‹ ë¢°ë„ ì ìˆ˜ (Anomaly Score) ì‚°ì¶œ\n",
        "        X_pred = autoencoder.predict(X_dynamic)\n",
        "        # ê° ì»´í¬ë„ŒíŠ¸(í–‰)ë³„ë¡œ ì›ë³¸ê³¼ ë³µì›ëœ ë°ì´í„°ì˜ MSE ê³„ì‚°\n",
        "        mse = np.mean(np.power(X_dynamic - X_pred, 2), axis=1)\n",
        "        df_sbom['ZT_Anomaly_Score'] = mse\n",
        "\n",
        "        # ì‹ ë¢°ë„ ì ìˆ˜(0~1)ë¡œ ë³€í™˜: MSEê°€ ë†’ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë‚®ìŒ\n",
        "        df_sbom['ZT_Trust_Score'] = 1 - (mse / np.max(mse))\n",
        "\n",
        "        # 4. AI ê¸°ë°˜ìœ¼ë¡œ 'ë¹„ì •ìƒ(Anomaly)' ì„ê³„ê°’ ìë™ ì •ì˜ ğŸ§ \n",
        "        # MSE ì ìˆ˜ì˜ í‰ê· (mean)ê³¼ í‘œì¤€í¸ì°¨(std)ë¥¼ ì‚¬ìš©í•œ í†µê³„ì  ê¸°ì¤€ ì„¤ì •\n",
        "        mean_mse = np.mean(mse)\n",
        "        std_mse = np.std(mse)\n",
        "\n",
        "        # 'ì •ìƒ' ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ì„ê³„ê°’ ìë™ ì„¤ì •: 3-ì‹œê·¸ë§ˆ(í‰ê·  + 3 * í‘œì¤€í¸ì°¨) ê·œì¹™ ì ìš©\n",
        "        # (ì´ ë°©ì‹ì€ ìˆ˜ë™ìœ¼ë¡œ network_calls < 5 ê°™ì€ ì„ì˜ì˜ ìˆ«ìë¥¼ ì •í•˜ëŠ” ëŒ€ì‹ , AIê°€ íŒë‹¨í•œ ë¹„ì •ìƒ ì ìˆ˜ë“¤ì˜ í†µê³„ì  ë¶„í¬ë¥¼ ë¶„ì„í•˜ì—¬ **ê°€ì¥ ê°ê´€ì ì¸ 'ì •ìƒ ê²½ê³„ì„ '**ì„ ìë™ìœ¼ë¡œ ì •ì˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.)\n",
        "        ANOMALY_THRESHOLD = mean_mse + 3 * std_mse\n",
        "\n",
        "        # 5. ìµœì¢… ë¹„ì •ìƒ(Anomaly) ë¶„ë¥˜\n",
        "        df_sbom['is_ZT_Anomaly'] = df_sbom['ZT_Anomaly_Score'] > ANOMALY_THRESHOLD\n",
        "\n",
        "        print(f\"\\nìë™ ì„¤ì •ëœ ë¹„ì •ìƒ ì„ê³„ê°’ (3-ì‹œê·¸ë§ˆ): {ANOMALY_THRESHOLD:.6f}\")\n",
        "        print(\"AIê°€ ìŠ¤ìŠ¤ë¡œ ì •ì˜í•œ 'ì •ìƒ' ê¸°ì¤€ì„ ë²—ì–´ë‚œ ì»´í¬ë„ŒíŠ¸ ëª©ë¡:\")\n",
        "        # is_ZT_Anomalyê°€ Trueì¸ ì»´í¬ë„ŒíŠ¸ ì¶œë ¥ (ìë™ìœ¼ë¡œ ì •ì˜ëœ ë¹„ì •ìƒ)\n",
        "        print(df_sbom[df_sbom['is_ZT_Anomaly']][['component', 'ZT_Anomaly_Score']].sort_values(by='ZT_Anomaly_Score', ascending=False).to_markdown(index=False, floatfmt=\".4f\"))\n",
        "        print(\"\\nAutoencoder ê¸°ë°˜ ì‹ ë¢°ë„ ì ìˆ˜ ì‚°ì¶œ ë° ë¹„ì •ìƒ ë¶„ë¥˜ ì™„ë£Œ.\")\n",
        "    else:\n",
        "        print(\"Error: 'network_calls' column not found in df_sbom. Cannot perform Autoencoder training and scoring.\")\n",
        "else:\n",
        "    print(\"Error: X_dynamic not found. Please ensure the previous cell executed successfully.\")"
      ],
      "metadata": {
        "id": "tqOsxasc8QJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac90f7b6-2f4b-4391-d23a-24f297111732"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder(ì‹ ë¢°ë„ ê²€ì¦) í•™ìŠµ ì‹œì‘...\n",
            "Autoencoder í•™ìŠµ ì™„ë£Œ.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7bc47bb1dda0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n",
            "\n",
            "ìë™ ì„¤ì •ëœ ë¹„ì •ìƒ ì„ê³„ê°’ (3-ì‹œê·¸ë§ˆ): 4.891804\n",
            "AIê°€ ìŠ¤ìŠ¤ë¡œ ì •ì˜í•œ 'ì •ìƒ' ê¸°ì¤€ì„ ë²—ì–´ë‚œ ì»´í¬ë„ŒíŠ¸ ëª©ë¡:\n",
            "| component   | ZT_Anomaly_Score   |\n",
            "|-------------|--------------------|\n",
            "\n",
            "Autoencoder ê¸°ë°˜ ì‹ ë¢°ë„ ì ìˆ˜ ì‚°ì¶œ ë° ë¹„ì •ìƒ ë¶„ë¥˜ ì™„ë£Œ.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì´ì „ ì…€ì—ì„œ ì„í¬íŠ¸í–ˆë‹¤ê³  ê°€ì •)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# ====================================================================\n",
        "# [ì˜¤ë¥˜ ìˆ˜ì • ë° ì‹¤í–‰ì„ ìœ„í•œ í•„ìˆ˜ ê°€ì • ë°ì´í„° ì„¤ì •]\n",
        "# ì´ì „ ì…€ì—ì„œ ìƒì„±/ë¡œë“œë˜ì—ˆë‹¤ê³  ê°€ì •í•œ ë³€ìˆ˜ë“¤ì„ ê°€ìƒìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "# ====================================================================\n",
        "\n",
        "# 1. df_sbom: ì·¨ì•½ì  ì—¬ë¶€ì™€ network_calls (Zero Trust í•™ìŠµì„ ìœ„í•œ ì„ì‹œ ì»¬ëŸ¼) í¬í•¨\n",
        "data = {\n",
        "    'component': ['req', 'num', 'flk', 'pan', 'djg', 'scp', 'tfl'],\n",
        "    'is_vulnerable': [1, 0, 1, 0, 1, 0, 0],  # 1: ì·¨ì•½, 0: ì•ˆì „ (Y_riskì˜ ì›ì²œ)\n",
        "    'network_calls': [4, 0, 8, 1, 5, 2, 3]   # ZT í•™ìŠµì„ ìœ„í•œ ì„ì‹œ ì»¬ëŸ¼\n",
        "}\n",
        "df_sbom = pd.DataFrame(data)\n",
        "\n",
        "# 2. X_text: í…ìŠ¤íŠ¸ ì„ë² ë”© (ê°€ìƒ: 10ì°¨ì› BERT ì„ë² ë”© ê°€ì •)\n",
        "np.random.seed(42)\n",
        "X_text = np.random.rand(len(df_sbom), 10)\n",
        "\n",
        "# 3. X_numeric: ë²„ì „ ìˆ˜ì¹˜í™” (ê°€ìƒ: 1ì°¨ì› ìˆ˜ì¹˜)\n",
        "X_numeric = np.random.rand(len(df_sbom), 1)\n",
        "\n",
        "# 4. ZT_Trust_Score (Colab ì…€ 2ì˜ ê²°ê³¼)\n",
        "# ì„ì‹œë¡œ ê³„ì‚°í•˜ì—¬ df_sbomì— ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "df_sbom['ZT_Trust_Score'] = 1 - (df_sbom['network_calls'] / df_sbom['network_calls'].max())\n",
        "# X_dynamic ë³€ìˆ˜ëŠ” ì´ ì…€ì—ì„œ ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, X_final ìƒì„±ì„ ìœ„í•´ ì´ì „ ì…€ì˜ ê²°ê³¼ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•¨.\n",
        "# ====================================================================\n",
        "\n",
        "\n",
        "# Colab ì…€ 3: ì·¨ì•½ì  ë° ìœ„í—˜ë„ ë¶„ë¥˜ AI (FNN)\n",
        "\n",
        "# 1. ëª¨ë“  íŠ¹ì§• ê²°í•© (í…ìŠ¤íŠ¸ ì„ë² ë”© í¬í•¨ - ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì¬ì‚¬ìš©)\n",
        "# X_final = [BERT ì„ë² ë”© | ë²„ì „ ìˆ˜ì¹˜ | ZT ì‹ ë¢°ë„ ì ìˆ˜ | NVD ë§¤ì¹­ ê²°ê³¼(is_vulnerable)]\n",
        "\n",
        "X_trust_feature = df_sbom[['ZT_Trust_Score', 'is_vulnerable']].values\n",
        "# ì¤‘ìš”: X_finalì€ ìµœì¢… ëª¨ë¸ì˜ ì…ë ¥ íŠ¹ì§•ì´ë¯€ë¡œ Y_risk(íƒ€ê²Ÿ)ë¥¼ í¬í•¨í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤!\n",
        "# Y_riskì™€ ì¤‘ë³µë˜ëŠ” is_vulnerable ì»¬ëŸ¼ì„ X_finalì—ì„œ ì œì™¸í•˜ê³ , ZT_Trust_Scoreë§Œ ì‚¬ìš©í•´ì•¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë§ìŠµë‹ˆë‹¤.\n",
        "# í•˜ì§€ë§Œ ì›ë³¸ ì½”ë“œë¥¼ ìµœëŒ€í•œ ìœ ì§€í•˜ê³  'is_vulnerable'ì„ ëª¨ë¸ ì…ë ¥ì— í¬í•¨ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\n",
        "# ì‹¤ì œë¡œëŠ” ì´ëŠ” ì •ë³´ ìœ ì¶œ(data leakage)ì— í•´ë‹¹í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì´ ê³¼ëŒ€í‰ê°€ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "# ì›ë³¸ ì½”ë“œëŒ€ë¡œ X_finalì„ ì¬ì •ì˜í•©ë‹ˆë‹¤.\n",
        "# ì›ë³¸ ì½”ë“œ: X_final = np.hstack([X_text, X_numeric, X_trust_feature])\n",
        "X_final = np.hstack([X_text, X_numeric, X_trust_feature])\n",
        "\n",
        "\n",
        "# 2. ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§\n",
        "Y_risk = df_sbom['is_vulnerable'] # ìµœì¢… ëª©í‘œ: ì·¨ì•½ ì—¬ë¶€ ë¶„ë¥˜ (0 ë˜ëŠ” 1)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_final, Y_risk, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler_final = StandardScaler()\n",
        "X_train_scaled = scaler_final.fit_transform(X_train)\n",
        "X_test_scaled = scaler_final.transform(X_test)\n",
        "\n",
        "\n",
        "# 3. FNN (Fully Connected Neural Network) ëª¨ë¸ ì •ì˜\n",
        "input_dim_final = X_train_scaled.shape[1]\n",
        "\n",
        "model_risk = tf.keras.Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(input_dim_final,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid') # ì·¨ì•½/ì•ˆì „ ì´ì§„ ë¶„ë¥˜\n",
        "])\n",
        "\n",
        "model_risk.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"ì·¨ì•½ì  ìœ„í—˜ë„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
        "model_risk.fit(\n",
        "    X_train_scaled, Y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_scaled, Y_test),\n",
        "    verbose=0\n",
        ")\n",
        "print(\"ì·¨ì•½ì  ìœ„í—˜ë„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\")\n",
        "\n",
        "# 4. ìµœì¢… ì˜ˆì¸¡ ë° ë¶„ì„\n",
        "# ëª¨ë¸ì€ ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¡œ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤. ì „ì²´ X_finalì„ ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤.\n",
        "X_final_scaled = scaler_final.transform(X_final)\n",
        "Y_pred_prob = model_risk.predict(X_final_scaled).flatten()\n",
        "df_sbom['AI_Risk_Probability'] = Y_pred_prob\n",
        "df_sbom['AI_Final_Risk'] = (Y_pred_prob > 0.5).astype(int) # 0.5 ì´ˆê³¼ ì‹œ ìœ„í—˜(1)ìœ¼ë¡œ ë¶„ë¥˜\n",
        "\n",
        "print(\"\\n--- ìµœì¢… AI ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ---\")\n",
        "print(df_sbom[['component', 'is_vulnerable', 'ZT_Trust_Score', 'AI_Risk_Probability', 'AI_Final_Risk']].to_markdown(index=False, floatfmt=(\".4f\", \".4f\", \".4f\", \".0f\")))"
      ],
      "metadata": {
        "id": "ynPjYLgJ8S-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef1450e-0211-45c6-d735-15fa2d2d5f1d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì·¨ì•½ì  ìœ„í—˜ë„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n",
            "ì·¨ì•½ì  ìœ„í—˜ë„ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7bc47bb565c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
            "\n",
            "--- ìµœì¢… AI ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½ ---\n",
            "| component   |   is_vulnerable |   ZT_Trust_Score |   AI_Risk_Probability |   AI_Final_Risk |\n",
            "|:------------|----------------:|-----------------:|----------------------:|----------------:|\n",
            "| req         |               1 |           0.5000 |                     0 |               0 |\n",
            "| num         |               0 |           1.0000 |                     0 |               0 |\n",
            "| flk         |               1 |           0.0000 |                     1 |               1 |\n",
            "| pan         |               0 |           0.8750 |                     0 |               0 |\n",
            "| djg         |               1 |           0.3750 |                     1 |               1 |\n",
            "| scp         |               0 |           0.7500 |                     0 |               0 |\n",
            "| tfl         |               0 |           0.6250 |                     0 |               0 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a62c896",
        "outputId": "6d3e9d39-87d0-40df-8442-a0277922799a"
      },
      "source": [
        "# Check the columns in df_sbom before running the next cell\n",
        "print(\"Columns in df_sbom before running cell ynPjYLgJ8S-0:\")\n",
        "print(df_sbom.columns)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in df_sbom before running cell ynPjYLgJ8S-0:\n",
            "Index(['component', 'version', 'purl', 'pid', 'description', 'is_vulnerable',\n",
            "       'cvss_score', 'cve_id'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab ì…€ 4: ì‹œê°í™” ë° ìœ„í—˜ë„ ë§¤íŠ¸ë¦­ìŠ¤\n",
        "\n",
        "# 1. ì‹ ë¢°ë„(Anomaly Score) vs. ì·¨ì•½ì  í™•ë¥ (Risk Probability) ë§¤íŠ¸ë¦­ìŠ¤\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x='ZT_Anomaly_Score', # ë¹„ì •ìƒ ì ìˆ˜(ë†’ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë‚®ìŒ)\n",
        "    y='AI_Risk_Probability',\n",
        "    data=df_sbom,\n",
        "    hue='AI_Final_Risk', # ìµœì¢… AI ì˜ˆì¸¡ ìœ„í—˜ ì—¬ë¶€ë¡œ ìƒ‰ìƒ êµ¬ë¶„\n",
        "    size='cvss_score', # ì› í¬ê¸°ë¡œ NVD ê¸°ë°˜ CVSS ì ìˆ˜ ë°˜ì˜\n",
        "    sizes=(20, 300),\n",
        "    palette=['blue', 'red'],\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "# ì œë¡œ íŠ¸ëŸ¬ìŠ¤íŠ¸ ìœ„í—˜ ì˜ì—­ í‘œì‹œ (ì˜ˆì‹œ)\n",
        "plt.axhline(0.5, color='orange', linestyle='--', label='Risk Probability Threshold')\n",
        "plt.axvline(np.percentile(df_sbom['ZT_Anomaly_Score'], 90), color='purple', linestyle=':', label='Top 10% Anomaly Threshold')\n",
        "\n",
        "plt.title('ì œë¡œ íŠ¸ëŸ¬ìŠ¤íŠ¸ ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤ (Anomaly vs. AI Risk)')\n",
        "plt.xlabel('ZT ë¹„ì •ìƒ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë‚®ìŒ)')\n",
        "plt.ylabel('AI ì˜ˆì¸¡ ìœ„í—˜ í™•ë¥ ')\n",
        "plt.legend(title='AI Final Risk')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. ìµœì¢… ìœ„í—˜ ì»´í¬ë„ŒíŠ¸ ëª©ë¡\n",
        "print(\"\\n--- AI ê¸°ë°˜ ì œë¡œ íŠ¸ëŸ¬ìŠ¤íŠ¸ ê³ ìœ„í—˜ ì»´í¬ë„ŒíŠ¸ ëª©ë¡ ---\")\n",
        "high_risk_components = df_sbom[\n",
        "    (df_sbom['AI_Final_Risk'] == 1) & (df_sbom['ZT_Trust_Score'] < 0.8) # ìœ„í—˜ ì˜ˆì¸¡ AND ì‹ ë¢°ë„ ì ìˆ˜ 80% ë¯¸ë§Œ\n",
        "].sort_values(by='AI_Risk_Probability', ascending=False)\n",
        "\n",
        "print(high_risk_components[['name', 'version', 'ZT_Trust_Score', 'AI_Risk_Probability', 'description']].head(10))"
      ],
      "metadata": {
        "id": "MtqtIk6R8VLl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "96fb6e3c-93f1-420c-aca2-5d0e60e2a50f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not interpret value `ZT_Anomaly_Score` for `x`. An entry with this name does not appear in `data`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2519369324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1. ì‹ ë¢°ë„(Anomaly Score) vs. ì·¨ì•½ì  í™•ë¥ (Risk Probability) ë§¤íŠ¸ë¦­ìŠ¤\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m sns.scatterplot(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ZT_Anomaly_Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# ë¹„ì •ìƒ ì ìˆ˜(ë†’ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë‚®ìŒ)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'AI_Risk_Probability'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/relational.py\u001b[0m in \u001b[0;36mscatterplot\u001b[0;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m ):\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m     p = _ScatterPlotter(\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/relational.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables, legend)\u001b[0m\n\u001b[1;32m    394\u001b[0m         )\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# information for numeric axes would be information about log scales.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_ordered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# alt., used DefaultDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# TODO Lots of tests assume that these are called to initialize the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_base.py\u001b[0m in \u001b[0;36massign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0;31m# to centralize / standardize data consumption logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"long\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0mplot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlotData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_core/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/seaborn/_core/data.py\u001b[0m in \u001b[0;36m_assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                     \u001b[0merr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"An entry with this name does not appear in `data`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not interpret value `ZT_Anomaly_Score` for `x`. An entry with this name does not appear in `data`."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}